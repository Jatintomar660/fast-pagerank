{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import scipy as sp\n",
    "import timeit\n",
    "import sys\n",
    "import networkx as nx\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def wikipr(M, eps=1.0e-8, d=0.85):\n",
    "    N = M.shape[1]\n",
    "    v = np.ra`ndom.rand(N, 1)\n",
    "    v = v / np.linalg.norm(v, 1)\n",
    "    last_v = np.ones((N, 1), dtype=np.float32) * 100\n",
    "    \n",
    "    while np.linalg.norm(v - last_v, 2) > eps:\n",
    "        last_v = v\n",
    "        v = d * np.matmul(M, v) + (1 - d) / N\n",
    "    return v    \n",
    "wikipr(A.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank_scipy(G, alpha=0.85, personalization=None,\n",
    "                   max_iter=100, tol=1.0e-6, weight='weight',\n",
    "                   dangling=None):\n",
    "    \"\"\"Return the PageRank of the nodes in the graph.\n",
    "\n",
    "    PageRank computes a ranking of the nodes in the graph G based on\n",
    "    the structure of the incoming links. It was originally designed as\n",
    "    an algorithm to rank web pages.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : graph\n",
    "      A NetworkX graph.  Undirected graphs will be converted to a directed\n",
    "      graph with two directed edges for each undirected edge.\n",
    "\n",
    "    alpha : float, optional\n",
    "      Damping parameter for PageRank, default=0.85.\n",
    "\n",
    "    personalization: dict, optional\n",
    "       The \"personalization vector\" consisting of a dictionary with a\n",
    "       key for every graph node and nonzero personalization value for each\n",
    "       node. By default, a uniform distribution is used.\n",
    "\n",
    "    max_iter : integer, optional\n",
    "      Maximum number of iterations in power method eigenvalue solver.\n",
    "\n",
    "    tol : float, optional\n",
    "      Error tolerance used to check convergence in power method solver.\n",
    "\n",
    "    weight : key, optional\n",
    "      Edge data key to use as weight.  If None weights are set to 1.\n",
    "\n",
    "    dangling: dict, optional\n",
    "      The outedges to be assigned to any \"dangling\" nodes, i.e., nodes without\n",
    "      any outedges. The dict key is the node the outedge points to and the dict\n",
    "      value is the weight of that outedge. By default, dangling nodes are given\n",
    "      outedges according to the personalization vector (uniform if not\n",
    "      specified) This must be selected to result in an irreducible transition\n",
    "      matrix (see notes under google_matrix). It may be common to have the\n",
    "      dangling dict to be the same as the personalization dict.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pagerank : dictionary\n",
    "       Dictionary of nodes with PageRank as value\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> G = nx.DiGraph(nx.path_graph(4))\n",
    "    >>> pr = nx.pagerank_scipy(G, alpha=0.9)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The eigenvector calculation uses power iteration with a SciPy\n",
    "    sparse matrix representation.\n",
    "\n",
    "    This implementation works with Multi(Di)Graphs. For multigraphs the\n",
    "    weight between two nodes is set to be the sum of all edge weights\n",
    "    between those nodes.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    pagerank, pagerank_numpy, google_matrix\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] A. Langville and C. Meyer,\n",
    "       \"A survey of eigenvector methods of web information retrieval.\"\n",
    "       http://citeseer.ist.psu.edu/713792.html\n",
    "    .. [2] Page, Lawrence; Brin, Sergey; Motwani, Rajeev and Winograd, Terry,\n",
    "       The PageRank citation ranking: Bringing order to the Web. 1999\n",
    "       http://dbpubs.stanford.edu:8090/pub/showDoc.Fulltext?lang=en&doc=1999-66&format=pdf\n",
    "    \"\"\"\n",
    "    import scipy.sparse\n",
    "\n",
    "    N = len(G)\n",
    "    if N == 0:\n",
    "        return {}\n",
    "\n",
    "    nodelist = G.nodes()\n",
    "    M = nx.to_scipy_sparse_matrix(G, nodelist=nodelist, weight=weight,\n",
    "                                  dtype=float)\n",
    "    #print (\"M=\", M)    \n",
    "    S = scipy.array(M.sum(axis=1)).flatten()\n",
    "    S[S != 0] = 1.0 / S[S != 0]\n",
    "    #print (\"S=\", S)\n",
    "    Q = scipy.sparse.spdiags(S.T, 0, *M.shape, format='csr')\n",
    "    #print (\"Q=\", Q)\n",
    "    M = Q * M\n",
    "    #print (\"M=\", M)\n",
    "    \n",
    "    # initial vector\n",
    "    x = scipy.repeat(1.0 / N, N)\n",
    "\n",
    "    # Personalization vector\n",
    "    if personalization is None:\n",
    "        p = scipy.repeat(1.0 / N, N)\n",
    "    else:\n",
    "        missing = set(nodelist) - set(personalization)\n",
    "        if missing:\n",
    "            raise NetworkXError('Personalization vector dictionary '\n",
    "                                'must have a value for every node. '\n",
    "                                'Missing nodes %s' % missing)\n",
    "        p = scipy.array([personalization[n] for n in nodelist],\n",
    "                        dtype=float)\n",
    "        p = p / p.sum()\n",
    "\n",
    "    # Dangling nodes\n",
    "    if dangling is None:\n",
    "        dangling_weights = p\n",
    "    else:\n",
    "        missing = set(nodelist) - set(dangling)\n",
    "        if missing:\n",
    "            raise NetworkXError('Dangling node dictionary '\n",
    "                                'must have a value for every node. '\n",
    "                                'Missing nodes %s' % missing)\n",
    "        # Convert the dangling dictionary into an array in nodelist order\n",
    "        dangling_weights = scipy.array([dangling[n] for n in nodelist],\n",
    "                                       dtype=float)\n",
    "        dangling_weights /= dangling_weights.sum()\n",
    "    is_dangling = scipy.where(S == 0)[0]\n",
    "\n",
    "    # power iteration: make up to max_iter iterations\n",
    "    for _ in range(max_iter):\n",
    "        xlast = x\n",
    "        x = alpha * (x * M + sum(x[is_dangling]) * dangling_weights) + \\\n",
    "            (1 - alpha) * p\n",
    "        # check convergence, l1 norm\n",
    "#         err = scipy.absolute(x - xlast).sum()\n",
    "#         if err < N * tol:\n",
    "        if sp.linalg.norm(x - xlast)<tol:\n",
    "            return dict(zip(nodelist, map(float, x)))\n",
    "    raise NetworkXError('pagerank_scipy: power iteration failed to converge '\n",
    "                        'in %d iterations.' % max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import scipy as sp\n",
    "import scipy.sparse as sprs\n",
    "import scipy.spatial\n",
    "import scipy.sparse.linalg\n",
    "\n",
    "def pagerank_power(G, p=0.85, max_iter=100,\n",
    "                   tol=1e-06, personalize=None, reverse=False):\n",
    "    \"\"\" Calculates pagerank given a csr graph\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "    G: a csr graph.\n",
    "    p: damping factor\n",
    "    max_iter: maximum number of iterations\n",
    "    personlize: if not None, should be an array with the size of the nodes\n",
    "                containing probability distributions.\n",
    "                It will be normalized automatically.\n",
    "    reverse: If true, returns the reversed-pagerank\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Pagerank Scores for the nodes\n",
    "\n",
    "    \"\"\"\n",
    "    # In Moler's algorithm, $G_{ij}$ represents the existences of an edge\n",
    "    # from node $j$ to $i$, while we have assumed the opposite!\n",
    "    if not reverse:\n",
    "        G = G.T\n",
    "\n",
    "    n, _ = G.shape\n",
    "    c = sp.asarray(G.sum(axis=0)).reshape(-1)\n",
    "\n",
    "    k = c.nonzero()[0]\n",
    "\n",
    "    D = sprs.csr_matrix((1 / c[k], (k, k)), shape=(n, n))\n",
    "\n",
    "    if personalize is None:\n",
    "        personalize = sp.ones(n)\n",
    "    personalize = personalize.reshape(n, 1)\n",
    "    e = (personalize / personalize.sum()) * n\n",
    "\n",
    "    z = (((1 - p) * (c != 0) + (c == 0)) / n)[sp.newaxis, :]\n",
    "    G = p * G.dot(D)\n",
    "\n",
    "    x = e / n\n",
    "    oldx = sp.zeros((n, 1))\n",
    "\n",
    "    iteration = 0\n",
    "\n",
    "    while sp.linalg.norm(x - oldx) > tol:\n",
    "        oldx = x\n",
    "        x = G.dot(x) + e.dot(z.dot(x))\n",
    "        iteration += 1\n",
    "        if iteration >= max_iter:\n",
    "            break\n",
    "    x = x / sum(x)\n",
    "\n",
    "    return x.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 1],\n",
       "        [1, 0, 0, ..., 0, 1, 0],\n",
       "        [0, 0, 0, ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 0, 0, ..., 0, 1, 0],\n",
       "        [0, 0, 1, ..., 0, 0, 1],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passed=True\n",
    "\n",
    "G_size = 1000\n",
    "p=0.2\n",
    "G = nx.fast_gnp_random_graph(G_size, p, seed=None, directed=True)\n",
    "# for e in G.edges():\n",
    "#      G[e[0]][e[1]]['weight']=sp.rand()\n",
    "A=nx.to_scipy_sparse_matrix(G)\n",
    "A.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47375453560089226"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeit.timeit(\n",
    "        lambda: pagerank_scipy(\n",
    "            G,\n",
    "            alpha=0.85,\n",
    "            tol=1e-3, max_iter=100),\n",
    "        number=5) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0213138805993367"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeit.timeit(\n",
    "        lambda: pagerank_power(\n",
    "            A, p=0.85, tol=1e-3, max_iter=100), number=5) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
